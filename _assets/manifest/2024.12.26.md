# Application Manifest
Version 1.0.3
This is the updated application manifest for the Plutomation utility application.

## What is Plutomation?


## Project Structure
```bash
plutomation/
|-- _assets/
|   |-- app-manifest.md
|   |-- imgs/
|   |-- plutomation-github-billboard.jpg
|-- app/
|   |-- __init__.py
|   |-- main.py
|   |-- tasks.py
|   |-- models.py
|   |-- schemas.py
|   |-- database.py
|   |-- routers/
|       |-- __init__.py
|       |-- job_search.py
|       |-- workflow_tracking.py
|-- celery_app/
|   |-- __init__.py
|-- tests/
|   |-- __init__.py
|   |-- test_routers.py
|   |-- test_tasks.py
|   |-- test_database.py
|-- README.md
```

## Project Standards
If you provide code updates, recommendations, etc. please follow these standards:

### Documentation
1. **Docstrings:** Added detailed docstrings to all modules and functions for clarity and maintainability.
2. **API Documentation:** Utilize FastAPI’s autogenerated Swagger UI and ReDoc for API exploration and testing. 
3. Add comments to clarify usage of environment variables

### Testing
1. **Unit Tests:** Introduced tests in the `tests/` directory using `pytest` and `pytest-asyncio` for robust asynchronous functionality.
2. **Coverage:** Ensure high test coverage for all routers, tasks, and database interactions.

### Code Quality
1. **Linting:** Enforce linting with `flake8` or `pylint` to maintain code style and detect issues early.
2. **Type Hints:** Added type hints to all functions and variables for better readability and type checking.

### Security
1. **Input Validation:** Enhanced input validation using FastAPI’s Pydantic models and schemas at both API and database levels.
2. **Logging:** Mask sensitive data in logs to prevent accidental exposure.

### Scalability
1. **Caching:** Implemented caching for frequently accessed endpoints using tools like Redis.
2. **Database Migrations:** Use Alembic for schema changes to ensure database consistency during deployment.

### Hosting
Adopting best practices in containerization, CI/CD, security, and monitoring to ensure a successful deployment into production.


---

## Assets ('_assets')
This directory serves as a repository for non-application files, such as static images and documentation like the app-manifest.md. 
This separation ensures the core application remains lightweight and focused.

## Application ('app/')
Contains the main business logic, APIs, and database interactions. It is the backbone of the utility, designed with FastAPI.

## Routers ('app/routers/')
Houses FastAPI routers for modular API endpoint definitions. This directory promotes separation of concerns and ease of maintenance.

### Initialize ('app/routers/__init__.py')
Contains imports for routers and metadata for easier integration.
```bash 
from .job_search import router as job_search_router
from .workflow_tracking import router as workflow_tracking_router

__all__ = [
    "job_search_router",
    "workflow_tracking_router"
]

__version__ = "1.1.0"
__author__ = "Plutomation Team"
```  

### Job Search ('app/routers/job_search.py')  
Defines endpoints for job application submission and triggers asynchronous job application tasks using Celery.  
```bash 
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import get_db
from app.models import JobApplication
from app.schemas import JobApplicationCreate, JobApplicationResponse
from celery_app import celery_app
import logging

logger = logging.getLogger("job_search")
router = APIRouter()

@router.post("/jobs/", response_model=JobApplicationResponse)
async def apply_to_job(
    job: JobApplicationCreate, db: AsyncSession = Depends(get_db)
):
    """
    Endpoint to create a job application entry in the database.
    """
    try:
        new_job = JobApplication(
            job_title=job.job_title, company_name=job.company_name, application_status="Applied"
        )
        db.add(new_job)
        await db.commit()
        await db.refresh(new_job)
        return new_job
    except Exception as e:
        logger.exception("Error applying to job")
        raise HTTPException(status_code=500, detail=f"Failed to apply: {str(e)}")

@celery_app.task
def apply_to_job_task(job_details: dict):
    """
    Celery task to asynchronously handle job applications.
    """
    logger.info(f"Processing job application: {job_details}")
    return {"status": "success", "job_title": job_details["job_title"]}

@router.post("/jobs/apply")
async def trigger_job_application(job_details: JobApplicationCreate):
    """
    Trigger an asynchronous task for a job application.
    """
    try:
        task = apply_to_job_task.delay(job_details.dict())
        return {"task_id": task.id, "status": "Job application task created"}
    except Exception as e:
        logger.exception("Error triggering job application task")
        raise HTTPException(status_code=500, detail=f"Failed to trigger job application: {str(e)}")

```  

### Workflow Tracking ('app/routers/workflow_tracking.py')
Implements endpoints to update workflow stages.
```bash 
# 'WORKFLOW TRACKING' - Implements endpoints to update workflow stages.
# Last updated on 12.26.2024

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import get_db
from app.models import WorkflowTracking
from app.schemas import WorkflowUpdate
import logging

logger = logging.getLogger("workflow_tracking")
router = APIRouter()

@router.put("/workflow/{workflow_id}")
async def update_workflow(
    workflow_id: int, update: WorkflowUpdate, db: AsyncSession = Depends(get_db)
):
    """
    Update the stage, notes, and completion status of a workflow entry.
    """
    try:
        workflow = await db.get(WorkflowTracking, workflow_id)
        if not workflow:
            logger.error(f"Workflow {workflow_id} not found.")
            raise HTTPException(status_code=404, detail="Workflow not found")

        workflow.stage = update.stage
        workflow.notes = update.notes
        workflow.completed = update.completed
        await db.commit()
        return {"message": "Workflow updated successfully"}
    except Exception as e:
        logger.exception("Error updating workflow")
        raise HTTPException(status_code=500, detail="Internal server error")

```  

### Initialize ('app/__init__.py')
Declare package-level metadata such as author and version. Import routers for application usage.
```bash 
from .routers import job_search_router, workflow_tracking_router

__version__ = "1.1.0"
__author__ = "Plutomation Team"
``` 
   
### Database ('app/database.py')
Handles database session management and engine creation.
```bash 
import os
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

# Environment variable: DATABASE_URL
# Specifies the connection string for the database. Defaults to a PostgreSQL database hosted locally.
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://user:password@localhost/plutomation_db")

# Environment variable: DB_ECHO
# If set to "true" (case insensitive), enables SQLAlchemy's logging of all executed SQL statements.
# Defaults to "false".
engine = create_async_engine(
    DATABASE_URL,
    echo=os.getenv("DB_ECHO", "false").lower() == "true",
    
    # Environment variable: DB_POOL_SIZE
    # Determines the size of the database connection pool. Defaults to 5.
    pool_size=int(os.getenv("DB_POOL_SIZE", "5")),
    
    # Environment variable: DB_MAX_OVERFLOW
    # Sets the number of connections that can be created beyond the pool_size. Defaults to 10.
    max_overflow=int(os.getenv("DB_MAX_OVERFLOW", "10")),
)

# Creates an async session maker for database interactions
async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

async def get_db():
    """
    Database session generator for dependency injection.
    Provides an async database session and ensures it is properly closed after use.
    """
    async with async_session() as session:
        yield session

```  

### Main ('app/main.py')
Bootstraps the FastAPI application and includes routers.
```bash
import os
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from app.routers import job_search_router, workflow_tracking_router
import logging
import time

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("plutomation")

# Load environment variables
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

app = FastAPI(debug=DEBUG_MODE, title="Plutomation API", version="1.1.0")

# Middleware for CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware for logging and performance metrics
class PerformanceMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
        logger.info(f"{request.method} {request.url} - {response.status_code} - {process_time:.2f}s")
        return response

app.add_middleware(PerformanceMiddleware)

@app.middleware("http")
async def error_handling_middleware(request: Request, call_next):
    try:
        return await call_next(request)
    except Exception as exc:
        logger.exception("Unhandled exception")
        return JSONResponse(status_code=500, content={"detail": "Internal server error"})

# Include routers
app.include_router(job_search_router, prefix="/job_search", tags=["Job Search"])
app.include_router(workflow_tracking_router, prefix="/workflow", tags=["Workflow"])

@app.get("/")
async def root():
    """
    Root endpoint to verify API status.
    """
    return {"message": "Plutomation Employment Utility API"}

```  
  

### Models ('app/models.py')
Defines database models for job applications and workflow tracking.
```bash 
from sqlalchemy import Column, Integer, String, DateTime, Boolean, func
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class JobApplication(Base):
    __tablename__ = "job_applications"

    id = Column(Integer, primary_key=True, index=True)
    job_title = Column(String, index=True)
    company_name = Column(String)
    application_status = Column(String, default="Pending")
    applied_at = Column(DateTime, default=func.now())

class WorkflowTracking(Base):
    __tablename__ = "workflow_tracking"

    id = Column(Integer, primary_key=True, index=True)
    stage = Column(String)
    notes = Column(String)
    completed = Column(Boolean, default=False)
    created_at = Column(DateTime, default=func.now())
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())


```  
  
### Schemas ('app/schemas.py')
Defines pydantic models for input and response validation.  
```bash  
from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum

class ApplicationStatus(str, Enum):
    PENDING = "Pending"
    APPLIED = "Applied"
    REJECTED = "Rejected"
    ACCEPTED = "Accepted"

class JobApplicationCreate(BaseModel):
    job_title: str = Field(..., min_length=1, max_length=100, description="Title of the job being applied for")
    company_name: str = Field(..., min_length=1, max_length=100, description="Name of the company")

class JobApplicationResponse(JobApplicationCreate):
    id: int
    application_status: ApplicationStatus
    applied_at: datetime

    class Config:
        orm_mode = True

class WorkflowUpdate(BaseModel):
    stage: str = Field(..., min_length=1, max_length=50, description="Current stage of the workflow")
    notes: str = Field(..., min_length=0, max_length=500, description="Optional notes for the workflow")
    completed: bool = Field(..., description="Whether the workflow is completed")


```  
  
### Tasks ('app/tasks.py')
Defines Celery tasks for job application processing and recruiter email notifications.
```bash 
from celery_app import celery_app
import logging

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_job_application(job_details):
    """Helper function to process job applications."""
    if not isinstance(job_details, dict) or 'job_title' not in job_details or 'company_name' not in job_details:
        raise ValueError("Invalid job details provided")

    logger.info(f"Processing job application for {job_details['job_title']} at {job_details['company_name']}")
    return {"status": "success", "job_title": job_details["job_title"]}

def process_recruiter_email(email_data):
    """Helper function to send recruiter emails."""
    if not isinstance(email_data, dict) or 'email' not in email_data:
        raise ValueError("Invalid email data provided")

    logger.info(f"Sending email to recruiter: {email_data['email']}")
    return {"status": "sent", "email": email_data["email"]}

@celery_app.task
def apply_to_job(job_details):
    try:
        result = process_job_application(job_details)
        return result
    except Exception as e:
        logger.exception("Failed to apply to job")
        return {"status": "error", "message": str(e)}

@celery_app.task
def send_recruiter_email(email_data):
    try:
        result = process_recruiter_email(email_data)
        return result
    except Exception as e:
        logger.exception("Failed to send recruiter email")
        return {"status": "error", "message": str(e)}

```  
  


## Celery App ('celery_app/')
Sets up the Celery instance with Redis as the broker and backend.
  
### Initialize ('celery_app/__init__.py')
Sets up the Celery instance with Redis as the broker and backend.
```bash 
import os
from celery import Celery

# Configure Celery instance with environment variables
celery_app = Celery(
    "plutomation",
    broker=os.getenv("CELERY_BROKER_URL", "redis://localhost:6379/0"),
    backend=os.getenv("CELERY_BACKEND_URL", "redis://localhost:6379/0")
)

# Update Celery configuration
celery_app.conf.update(
    task_serializer="json",  # Serialize tasks in JSON format for readability
    result_serializer="json",  # Serialize results in JSON format for consistency
    accept_content=["json"],  # Restrict accepted content types to JSON for security
    timezone=os.getenv("CELERY_TIMEZONE", "UTC"),  # Set timezone, defaulting to UTC
    enable_utc=True,  # Ensure UTC is enabled for consistency across deployments
)

# Add comments to clarify usage of environment variables
# CELERY_BROKER_URL: URL for the message broker (e.g., Redis, RabbitMQ).
# CELERY_BACKEND_URL: URL for the result backend (e.g., Redis, database).
# CELERY_TIMEZONE: Timezone for task scheduling and timestamps.
``` 
  
## Tests ('tests/')  
A directory for test cases.

### Initialize ('tests/__init__.py')
Provide a 150 character description for the purpose of this file.
```bash
Provide code for intialization file
```

### Test Routers ('tests/test.routers.py')
Provide a 150 character description for the purpose of this file.
```bash
Provide code for intialization file
```

### Test Tasks ('tests/test.tasks.py')
Provide a 150 character description for the purpose of this file.
```bash
Provide code for intialization file
```

### Test Database ('tests/test.database.py')
Provide a 150 character description for the purpose of this file.
```bash
Provide code for intialization file
```